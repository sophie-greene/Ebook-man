{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "from pprint import pprint   # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/deerwester.dict') # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 11, u'graph': 10, u'system': 6, u'trees': 9, u'eps': 8, u'computer': 1, u'survey': 5, u'user': 7, u'human': 2, u'time': 4, u'interface': 0, u'response': 3}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec) # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (6, 1), (7, 1), (8, 1)], [(2, 1), (6, 2), (8, 1)], [(3, 1), (4, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(5, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus) # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "#from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "corpus = corpora.MmCorpus('/tmp/deerwester.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271602), (1, -0.070027665279000256)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"This is a foo bar sentence .\"\n",
    "s2 = \"This sentence is similar to a foo bar sentence .\"\n",
    "s3 = \"What is this string ? Totally not related to the other two lines .\"\n",
    "\n",
    "cosine_sim(s1, s2) # Should give high cosine similarity\n",
    "cosine_sim(s1, s3) # Shouldn't give high cosine similarity value\n",
    "cosine_sim(s2, s3) # Shouldn't give high cosine similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def similarity(text1,text2):\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8616404368553293"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This is a foo bar sentence .'\n",
    "text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "similarity(text1,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 's' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4c83eb574f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misbntools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0misbntools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#using isbntools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetIMeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/isbntools/app.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# inject isbntools dependencies on isbnlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# <-- first import after lib (IMPORTANT)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_initapp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONF_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCACHE_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# isbntools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/isbntools/_initapp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0mcache_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONF_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCACHE_FOLDER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0misbnlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoversCache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_covers_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCoversCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/isbnlib/dev/_coverscache.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cachepath)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indexpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShelveCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indexpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAXLEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/isbnlib/dev/_shelvecache.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 's' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import isbntools\n",
    "import isbntools.app as app\n",
    "#using isbntools\n",
    "def getIMeta(title,str(author)):\n",
    "    \n",
    "    print query\n",
    "    isbn = app.isbn_from_words(query)    \n",
    "    \n",
    "        \n",
    "    if isbn != None:\n",
    "        meta=isbntools.app.meta(isbn)\n",
    "        author=str(meta['Authors'][0])\n",
    "        title=str(meta['Title'])\n",
    "        fileName = author+\" - \"+title\n",
    "        return [fileName,meta]\n",
    "    else:\n",
    "        return \"Query Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from isbnlib import meta\n",
    "import isbnlib\n",
    "from isbnlib.config import add_apikey\n",
    "from isbnlib.dev.helpers import fmtbib\n",
    "from comFuncs import *\n",
    "APIKEY=\"2E7FE5A4\"\n",
    "import os\n",
    "import string\n",
    "#SERVICE = 'isbndb'\n",
    "#register your key\n",
    "#add_apikey(SERVICE, APIKEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8837"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=\"/media/some/sophie-hhd/transmitt/fic/#Calibre Fiction/\"\n",
    "dd=\"/media/some/sophie-hhd/transmitt/fic/Fictional Literature/\"\n",
    "files=! find \"$d\" \"$dd\" -type f -iname \"*.epub\"\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getISBNmeta(title,author):\n",
    "    #isbn=isbnlib.isbn_from_words(title+\" by \" +author)\n",
    "    try:\n",
    "        return isbnlib.goom(title+\" \" +author)\n",
    "    except:\n",
    "        print \"service unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getFileIsbnMeta(f,smscr):\n",
    "    fp,ff= os.path.split(f)\n",
    "    fn, fext = os.path.splitext(ff)\n",
    "    seri=''\n",
    "    sm=0\n",
    "    mtd=None\n",
    "    auth=fn.split()[-2:]\n",
    "    tit=fn.split()[:-2]\n",
    "    if fn.count(' - ')==1:\n",
    "        tit,auth=fn.split(' - ')\n",
    "    elif fn.count(' - ')==2:\n",
    "        seri,tit,auth=fn.split(' - ')\n",
    "        \n",
    "    author=''.join(l for l in ' '.join(auth.split()) if l in string.ascii_letters+\" .\")\n",
    "    if tit.count('_ ')==1:\n",
    "            tit=' '.join(tit.split('_ ')[0].split())\n",
    "    title=''.join(l for l in ' '.join(tit.split()) if l in string.ascii_letters+string.digits+\" .,&$£\")\n",
    "    #print title,'..... ....',author\n",
    "    metad =getISBNmeta(title,str(author))\n",
    "    if metad !=None and len(metad)>0: \n",
    "        for md in metad:\n",
    "            #print md\n",
    "            authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(author))\n",
    "            titsim=similarity(md['Title'].encode('ascii','ignore'),str(title))\n",
    "            #print titsim,authsim\n",
    "            if titsim+authsim >sm:\n",
    "                sm=titsim+authsim\n",
    "                mtd=md\n",
    "    #if no data found switch title and author\n",
    "    metad =getISBNmeta(author,str(title))\n",
    "    if metad !=None and len(metad)>0: \n",
    "        for md in metad:\n",
    "            authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(title))\n",
    "            titsim=similarity(md['Title'].encode('ascii','ignore'),str(author))\n",
    "            #print titsim,authsim\n",
    "            if titsim+authsim >sm:\n",
    "                sm=titsim+authsim\n",
    "                mtd=md\n",
    "    # if no data found try seri, auth ,title  combinaton\n",
    "    if seri !='':\n",
    "        metad =getISBNmeta(title,str(seri))\n",
    "        if metad !=None and len(metad)>0: \n",
    "            for md in metad:\n",
    "                authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(seri))\n",
    "                titsim=similarity(md['Title'].encode('ascii','ignore'),str(title))\n",
    "                #print titsim,authsim\n",
    "                if titsim+authsim >sm:\n",
    "                    sm=titsim+authsim\n",
    "                    mtd=md\n",
    "        metad =getISBNmeta(seri,str(author))\n",
    "        if metad !=None and len(metad)>0: \n",
    "            for md in metad:\n",
    "                authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(author))\n",
    "                titsim=similarity(md['Title'].encode('ascii','ignore'),str(seri))\n",
    "                #print titsim,authsim\n",
    "                if titsim+authsim >sm:\n",
    "                    sm=titsim+authsim\n",
    "                    mtd=md    \n",
    "        metad =getISBNmeta(seri,str(title))\n",
    "        if metad !=None and len(metad)>0: \n",
    "            for md in metad:\n",
    "                authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(title))\n",
    "                titsim=similarity(md['Title'].encode('ascii','ignore'),str(seri))\n",
    "                #print titsim,authsim\n",
    "                if titsim+authsim >sm:\n",
    "                    sm=titsim+authsim\n",
    "                    mtd=md\n",
    "        metad =getISBNmeta(author,str(seri))\n",
    "        if metad !=None and len(metad)>0: \n",
    "            for md in metad:\n",
    "                authsim=similarity(md['Authors'][0].encode('ascii','ignore'),str(seri))\n",
    "                titsim=similarity(md['Title'].encode('ascii','ignore'),str(author))\n",
    "                #print titsim,authsim\n",
    "                if titsim+authsim >sm:\n",
    "                    sm=titsim+authsim\n",
    "                    mtd=md               \n",
    "    if sm >smscr and mtd !=None:\n",
    "        return fn,fp,mtd,sm\n",
    "    return \"NO meta\"       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/some/sophie-hhd/transmitt/fic/Fictional Literature/Barnes and Noble Classics Series Collection/The Inferno - Dante Alighieri.epub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The Inferno - Dante Alighieri',\n",
       " '/media/some/sophie-hhd/transmitt/fic/Fictional Literature/Barnes and Noble Classics Series Collection',\n",
       " {'Authors': [u'Dante Alighieri'],\n",
       "  'ISBN-13': u'9781631061493',\n",
       "  'Language': u'en',\n",
       "  'Publisher': u'Race Point Pub',\n",
       "  'Title': u'The Inferno',\n",
       "  'Year': u'2015'},\n",
       " 1.9999999999999996)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=files[-4053]\n",
    "print f\n",
    "getFileIsbnMeta(f,.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
